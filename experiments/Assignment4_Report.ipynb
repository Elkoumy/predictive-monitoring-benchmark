{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 4 : Predictive Process Monitoring</center>\n",
    "\n",
    "## <center>Gamal Elkoumy</center>\n",
    "### <center>University of Tartu, 2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we report the our solution to the predictive process monitoring presented here https://courses.cs.ut.ee/LTAT.05.025/2021_spring/uploads/Main/2021Homework4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution GitHub Repository\n",
    "Our solution is available using the following URL.\n",
    "https://github.com/Elkoumy/predictive-monitoring-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "(1 point) \n",
    "\n",
    "As part of the log preprocessing, it is necessary to categorize the process traces as\n",
    "deviant or regular. This log contains a column called SLA. it is a \"case attribute,\" which indicates\n",
    "how many minutes each case must complete. You must create a new column in the log that\n",
    "contains a case attribute called label, which contains a value of 1 for deviant cases or 0 for\n",
    "regular ones. This column's value is 0 if the duration of the case (in minutes) is less than or equal\n",
    "to the SLA; otherwise, this column's value must be 1 (the SLA has not been met). NB! If there are\n",
    "cases that do not have SLA, categorize them as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Gamal Elkoumy\\PhD\\OneDrive - Tartu Ülikool\\Courses\\Process Mining\\Assignment4\\predictive-monitoring-benchmark\\data\\turnaround_anon_sla.csv')\n",
    "\n",
    "#converting datatypes , timestamps\n",
    "df.start_timestamp= pd.to_datetime(df.start_timestamp,utc=True)\n",
    "df.end_timestamp= pd.to_datetime(df.end_timestamp,utc=True)\n",
    "\n",
    "df=df.sort_values(['start_timestamp']).reset_index()\n",
    "df=df.drop('index',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1\"\"\"\n",
    "\n",
    "#calculating the start time and end time of every case\n",
    "df['case_end_time']=df.groupby(['caseid']).end_timestamp.transform('max')\n",
    "df['case_start_time']=df.groupby(['caseid']).start_timestamp.transform('min')\n",
    "\n",
    "#calculating case duration in minutes ( the same time unit as the SLA)\n",
    "df['duration']=(df.case_end_time-df.case_start_time).astype('timedelta64[m]')\n",
    "\n",
    "\n",
    "#creating the label column\n",
    "df['label']=1\n",
    "df.loc[df.duration<=df['SLA MIN'],'label']=0\n",
    "df.loc[df['SLA MIN'].isna(),'label']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "(2 points) \n",
    "\n",
    "Add a column to the event log that captures the WIP of the process at the moment\n",
    "where the last eventin the prefix occurs. Remember that the WIP refers to the number of active\n",
    "cases, meaning the number of cases that have started but not yet completed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we define a funtion that performs the estimation of wip for each activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_wip(row, case_times):\n",
    "    wip=0\n",
    "    #started before start and ended after end\n",
    "    #started after start and ended before end\n",
    "    #started before start and ended before end\n",
    "    #started before end and ended after end\n",
    "    wip=case_times.loc[(case_times.case_start_time<= row.start_timestamp) & (case_times.case_end_time>=row.end_timestamp) |\n",
    "                       (case_times.case_start_time >= row.start_timestamp) & (case_times.case_end_time <= row.end_timestamp)|\n",
    "                       (case_times.case_start_time <= row.start_timestamp) & (case_times.case_end_time >= row.start_timestamp)|\n",
    "                       (case_times.case_start_time <= row.end_timestamp) & (case_times.case_end_time >= row.end_timestamp)\n",
    "                       ].shape[0]\n",
    "\n",
    "    return wip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We then use the pandas apply function to execute the count_wip function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2\"\"\"\n",
    "case_times= pd.DataFrame()\n",
    "case_times['case_end_time']=df.groupby(['caseid']).end_timestamp.max()\n",
    "case_times['case_start_time']=df.groupby(['caseid']).start_timestamp.min()\n",
    "case_times=case_times.reset_index()\n",
    "\n",
    "df['WIP']=df.apply(count_wip,case_times=case_times ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We export the result in order to use it separately to optimize the model parameters as we will mention later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'caseid': 'Case ID','activity':'Activity', 'start_timestamp':'time:timestamp'})\n",
    "df.to_csv(r'C:\\Gamal Elkoumy\\PhD\\OneDrive - Tartu Ülikool\\Courses\\Process Mining\\Assignment4\\predictive-monitoring-benchmark\\experiments\\experiment_log\\turnaround_anon_sla_renamed.csv',index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a preprocessing for the next step, we prepare the data for the train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test\n",
    "def split_data_strict(data, train_ratio, split=\"temporal\"):\n",
    "    # split into train and test using temporal split and discard events that overlap the periods\n",
    "    data = data.sort_values(['time:timestamp', 'Activity'], ascending=True, kind='mergesort')\n",
    "    grouped = data.groupby('Case ID')\n",
    "    start_timestamps = grouped['time:timestamp'].min().reset_index()\n",
    "    start_timestamps = start_timestamps.sort_values('time:timestamp', ascending=True, kind='mergesort')\n",
    "    train_ids = list(start_timestamps['Case ID'])[:int(train_ratio*len(start_timestamps))]\n",
    "    train = data[data['Case ID'].isin(train_ids)].sort_values(['time:timestamp', 'Activity'], ascending=True, kind='mergesort')\n",
    "    test = data[~data['Case ID'].isin(train_ids)].sort_values(['time:timestamp', 'Activity'], ascending=True, kind='mergesort')\n",
    "    split_ts = test['time:timestamp'].min()\n",
    "    train = train[train['time:timestamp'] < split_ts]\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split into train and test\"\"\"\n",
    "train_ratio = 0.8\n",
    "n_splits = 2\n",
    "random_state = 22\n",
    "\n",
    "train, test = split_data_strict(df, train_ratio, split=\"temporal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "(4 points) \n",
    "\n",
    "Currently, the work proposed by Taineema et al. performs the extraction of the prefixes\n",
    "of the traces registered in the log to train the classification models. For large logs, this approach\n",
    "leads to an increase in the dimensionality of the models' input (too many features) without\n",
    "necessarily improving its precision, especially in cases in which the event traces are very long.\n",
    "You must modify this technique to extract subsequences of size n (n-grams), where n is a userdefined parameter, instead of encoding entire prefixes. An n-gram is a contiguous sequence of n\n",
    "items from a given trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we define the function that calculates the n-grams. The following function calculates the prefixes using the n-grams for every case separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(data, ngram_size):\n",
    "    result=pd.DataFrame()\n",
    "\n",
    "\n",
    "    for idx in range(0,data.shape[0]- ngram_size +1):\n",
    "\n",
    "        prefix=data.iloc[idx:idx+ngram_size].copy()\n",
    "        prefix=prefix.reset_index()\n",
    "\n",
    "        prefix['Case ID']=prefix['Case ID']+'_'+str(idx)\n",
    "\n",
    "        result=pd.concat([result,prefix])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a helper function, we adapted the following method to the new label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_ratio(data):\n",
    "    class_freqs = data['label'].value_counts()\n",
    "    return class_freqs[1] / class_freqs.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We then follow the same CV method as the practice session 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def get_stratified_split_generator(data, n_splits=5, shuffle=True, random_state=22):\n",
    "    grouped_firsts = data.groupby('Case ID', as_index=False).first()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    for train_index, test_index in skf.split(grouped_firsts, grouped_firsts['label']):\n",
    "        current_train_names = grouped_firsts['Case ID'][train_index]\n",
    "        train_chunk = data[data['Case ID'].isin(current_train_names)].sort_values('time:timestamp', ascending=True, kind='mergesort')\n",
    "        test_chunk = data[~data['Case ID'].isin(current_train_names)].sort_values('time:timestamp', ascending=True, kind='mergesort')\n",
    "        yield (train_chunk, test_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare chunks for CV\n",
    "dt_prefixes = []\n",
    "class_ratios = []\n",
    "min_prefix_length = 1\n",
    "ngram_size=5\n",
    "\n",
    "for train_chunk, test_chunk in get_stratified_split_generator(train, n_splits=n_splits):\n",
    "    class_ratios.append(get_class_ratio(train_chunk))\n",
    "    # generate data where each prefix is a separate instance\n",
    "    dt_prefixes.append(generate_prefix_data(test_chunk, ngram_size))\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "(3 points) \n",
    "\n",
    "Test the results of your modifications with the Turnaround process event log using\n",
    "cluster bucketing, index encoding, and the XGboost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Optimization\n",
    "\n",
    "Taineema et al provide a method for optimizing the model parameters for predictive process monitoring. The file <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/optimize_params.py\">optimize_params.py</a> performs the parameter optimization. We adopted the file by adding the required parameters for the input event log \"turnaround_anon_sla.csv\".\n",
    "\n",
    "We needed also to perform adaptations in the file <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/dataset_confs.py\">dataset_confs.py</a> in order to enable the parameter tuning for the dataset \"turnaround_anon_sla.csv\". \n",
    "\n",
    "We used the following command to execute the optimizer:\n",
    " python optimize_params.py turnaround_anon_sla_renamed optimizer_log 10  cluster index xgboost \n",
    "\n",
    "The output of the optimizer could be found in the folder <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/optimizer_log\">optimizer_log</a>. Also, the optimial parameters are in the pickle file <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/optimizer_log/optimal_params_xgboost_turnaround_anon_sla_renamed_cluster_index.pickle\">optimal_params_xgboost_turnaround_anon_sla_renamed_cluster_index.pickle</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Bucketing\n",
    "\n",
    "We used the Cluster Bucketing methods provided in practice session 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BucketFactory\n",
    "\n",
    "# encoding_method = \"last\", \"agg\", \"index\"\n",
    "# Bucketing prefixes based on control flow\n",
    "bucketer_args = {'encoding_method': 'last',\n",
    "                 'case_id_col': 'Case ID',\n",
    "                 'cat_cols':['Activity'],\n",
    "                 'num_cols':[],\n",
    "                 'random_state':random_state}\n",
    "\n",
    "cv_iter = 0\n",
    "dt_test_prefixes = dt_prefixes[cv_iter]\n",
    "dt_train_prefixes = pd.DataFrame()\n",
    "for cv_train_iter in range(n_splits):\n",
    "    if cv_train_iter != cv_iter:\n",
    "        dt_train_prefixes = pd.concat([dt_train_prefixes, dt_prefixes[cv_train_iter]], axis=0)\n",
    "\n",
    "\n",
    "cv_iter = 0\n",
    "dt_test_prefixes = dt_prefixes[cv_iter]\n",
    "dt_train_prefixes = pd.DataFrame()\n",
    "for cv_train_iter in range(n_splits):\n",
    "    if cv_train_iter != cv_iter:\n",
    "        dt_train_prefixes = pd.concat([dt_train_prefixes, dt_prefixes[cv_train_iter]], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Case00_6', 'Case01_18', 'Case02_10', 'Case03_20', 'Case04_14',\n",
      "       'Case08_9', 'Case10_12', 'Case11_18', 'Case13_18', 'Case17_7',\n",
      "       'Case19_5', 'Case22_23', 'Case24_11', 'Case28_14', 'Case30_11',\n",
      "       'Case31_24', 'Case34_20', 'Case35_23', 'Case40_13', 'Case41_19',\n",
      "       'Case42_16', 'Case44_15', 'Case47_2', 'Case48_13', 'Case51_16'],\n",
      "      dtype='object', name='Case ID')\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ************* Performing Cluster Bucketing ***************\"\"\"\n",
    "\n",
    "#bucket_methods = \"single\", \"prefix\", \"state\", \"cluster\", \"knn\"\n",
    "bucket_method = 'cluster'\n",
    "if bucket_method == \"cluster\":\n",
    "    bucketer_args[\"n_clusters\"] = 3\n",
    "bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "bucket_assignments_test = bucketer.predict(dt_test_prefixes)\n",
    "\n",
    "\"\"\" Train buckets\"\"\"\n",
    "bucket_number = 2\n",
    "bucket_indexes = dt_train_prefixes.groupby('Case ID').first().index\n",
    "bucket_indexes = bucket_indexes[bucket_assignments_train == bucket_number]\n",
    "print(bucket_indexes)\n",
    "bucket_data = dt_train_prefixes[dt_train_prefixes['Case ID'].isin(bucket_indexes)]\n",
    "\n",
    "def get_label_numeric(data):\n",
    "    y = data.groupby('Case ID').first()['label'] # one row per case\n",
    "    return y\n",
    "train_y = get_label_numeric(bucket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test Buckets\"\"\"\n",
    "\n",
    "bucket_indexes = dt_test_prefixes.groupby('Case ID').first().index\n",
    "bucket_indexes = bucket_indexes[bucket_assignments_test == bucket_number]\n",
    "bucket_data_test = dt_test_prefixes[dt_test_prefixes['Case ID'].isin(bucket_indexes)]\n",
    "test_y = get_label_numeric(bucket_data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Encoding Indexing\n",
    "We used the Encoding Indexing methods provided in practice session 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ************* Perfroming Index Encoding ******************\"\"\"\n",
    "import EncoderFactory\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "cls_encoder_args = {'case_id_col': 'Case ID',\n",
    "                    'static_cat_cols': [],\n",
    "                    'static_num_cols': [],\n",
    "                    'dynamic_cat_cols': ['Activity'],\n",
    "                    'dynamic_num_cols': [\"WIP\"],\n",
    "                    'fillna': True}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "}\n",
    "\n",
    "methods = encoding_dict['index']\n",
    "\n",
    "feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "\n",
    "encoding = feature_combiner.fit_transform(bucket_data, train_y)\n",
    "\n",
    "pd.DataFrame(encoding).to_csv('encoding.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Training\n",
    "In the following code, we use the model parameters optimized as mentioned above. The output of the optimizer could be found in the folder <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/optimizer_log\">```optimizer_log```</a>. Also, the optimial parameters are in the pickle file <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/optimizer_log/optimal_params_xgboost_turnaround_anon_sla_renamed_cluster_index.pickle\">```optimal_params_xgboost_turnaround_anon_sla_renamed_cluster_index.pickle```</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:36:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"n_clusters\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:36:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "The ROC AUC is : 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"******* Perfroming training **************\"\"\"\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "model_parameters=pd.read_pickle(r'C:\\Gamal Elkoumy\\PhD\\OneDrive - Tartu Ülikool\\Courses\\Process Mining\\Assignment4\\predictive-monitoring-benchmark\\experiments\\optimizer_log\\optimal_params_xgboost_turnaround_anon_sla_renamed_cluster_index.pickle')\n",
    "\n",
    "model= xgb.XGBClassifier(**model_parameters)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('encoder', feature_combiner), ('cls', model)])\n",
    "pipeline.fit(bucket_data, train_y)\n",
    "\n",
    "preds_pos_label_idx = np.where(model.classes_ == 1)[0][0]\n",
    "preds = pipeline.predict_proba(bucket_data_test)[:,preds_pos_label_idx]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(test_y, preds)\n",
    "print(\"The ROC AUC is : %s\"%(score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
