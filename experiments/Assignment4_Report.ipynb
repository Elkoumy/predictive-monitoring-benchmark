{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 4 : Predictive Process Monitoring</center>\n",
    "\n",
    "## <center>Gamal Elkoumy</center>\n",
    "### <center>University of Tartu, 2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we report the our solution to the predictive process monitoring presented here https://courses.cs.ut.ee/LTAT.05.025/2021_spring/uploads/Main/2021Homework4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution GitHub Repository\n",
    "Our solution is available using the following URL.\n",
    "https://github.com/Elkoumy/predictive-monitoring-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "(1 point) \n",
    "\n",
    "As part of the log preprocessing, it is necessary to categorize the process traces as\n",
    "deviant or regular. This log contains a column called SLA. it is a \"case attribute,\" which indicates\n",
    "how many minutes each case must complete. You must create a new column in the log that\n",
    "contains a case attribute called label, which contains a value of 1 for deviant cases or 0 for\n",
    "regular ones. This column's value is 0 if the duration of the case (in minutes) is less than or equal\n",
    "to the SLA; otherwise, this column's value must be 1 (the SLA has not been met). NB! If there are\n",
    "cases that do not have SLA, categorize them as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Gamal Elkoumy\\PhD\\OneDrive - Tartu Ülikool\\Courses\\Process Mining\\Assignment4\\predictive-monitoring-benchmark\\data\\turnaround_anon_sla.csv')\n",
    "\n",
    "#converting datatypes , timestamps\n",
    "df.start_timestamp= pd.to_datetime(df.start_timestamp,utc=True)\n",
    "df.end_timestamp= pd.to_datetime(df.end_timestamp,utc=True)\n",
    "\n",
    "df=df.sort_values(['start_timestamp']).reset_index()\n",
    "df=df.drop('index',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1\"\"\"\n",
    "\n",
    "#calculating the start time and end time of every case\n",
    "df['case_end_time']=df.groupby(['caseid']).end_timestamp.transform('max')\n",
    "df['case_start_time']=df.groupby(['caseid']).start_timestamp.transform('min')\n",
    "\n",
    "#calculating case duration in minutes ( the same time unit as the SLA)\n",
    "df['duration']=(df.case_end_time-df.case_start_time).astype('timedelta64[m]')\n",
    "\n",
    "\n",
    "#creating the label column\n",
    "df['label']=1\n",
    "df.loc[df.duration<=df['SLA MIN'],'label']=0\n",
    "df.loc[df['SLA MIN'].isna(),'label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "(2 points) \n",
    "\n",
    "Add a column to the event log that captures the WIP of the process at the moment\n",
    "where the last eventin the prefix occurs. Remember that the WIP refers to the number of active\n",
    "cases, meaning the number of cases that have started but not yet completed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we define a funtion that performs the estimation of wip for each activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_wip(row, case_times):\n",
    "    wip=0\n",
    "    #started before start and ended after end\n",
    "    #started after start and ended before end\n",
    "    #started before start and ended before end\n",
    "    #started before end and ended after end\n",
    "    wip=case_times.loc[(case_times.case_start_time<= row.start_timestamp) & (case_times.case_end_time>=row.end_timestamp) |\n",
    "                       (case_times.case_start_time >= row.start_timestamp) & (case_times.case_end_time <= row.end_timestamp)|\n",
    "                       (case_times.case_start_time <= row.start_timestamp) & (case_times.case_end_time >= row.start_timestamp)|\n",
    "                       (case_times.case_start_time <= row.end_timestamp) & (case_times.case_end_time >= row.end_timestamp)\n",
    "                       ].shape[0]\n",
    "\n",
    "    return wip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We then use the pandas apply function to execute the count_wip function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2\"\"\"\n",
    "case_times= pd.DataFrame()\n",
    "case_times['case_end_time']=df.groupby(['caseid']).end_timestamp.max()\n",
    "case_times['case_start_time']=df.groupby(['caseid']).start_timestamp.min()\n",
    "case_times=case_times.reset_index()\n",
    "\n",
    "df['WIP']=df.apply(count_wip,case_times=case_times ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We export the result in order to use it separately to optimize the model parameters as we will mention later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'caseid': 'Case ID','activity':'Activity', 'start_timestamp':'time:timestamp'})\n",
    "df.to_csv(r'C:\\Gamal Elkoumy\\PhD\\OneDrive - Tartu Ülikool\\Courses\\Process Mining\\Assignment4\\predictive-monitoring-benchmark\\experiments\\experiment_log\\turnaround_anon_sla_renamed.csv',index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a preprocessing for the next step, we prepare the data for the train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test\n",
    "def split_data_strict(data, train_ratio, split=\"temporal\"):\n",
    "    # split into train and test using temporal split and discard events that overlap the periods\n",
    "    data = data.sort_values(['time:timestamp', 'Activity'], ascending=True, kind='mergesort')\n",
    "    grouped = data.groupby('Case ID')\n",
    "    start_timestamps = grouped['time:timestamp'].min().reset_index()\n",
    "    start_timestamps = start_timestamps.sort_values('time:timestamp', ascending=True, kind='mergesort')\n",
    "    train_ids = list(start_timestamps['Case ID'])[:int(train_ratio*len(start_timestamps))]\n",
    "    train = data[data['Case ID'].isin(train_ids)].sort_values(['time:timestamp', 'Activity'], ascending=True, kind='mergesort')\n",
    "    test = data[~data['Case ID'].isin(train_ids)].sort_values(['time:timestamp', 'Activity'], ascending=True, kind='mergesort')\n",
    "    split_ts = test['time:timestamp'].min()\n",
    "    train = train[train['time:timestamp'] < split_ts]\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Split into train and test\"\"\"\n",
    "train_ratio = 0.8\n",
    "n_splits = 2\n",
    "random_state = 22\n",
    "\n",
    "train, test = split_data_strict(df, train_ratio, split=\"temporal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "(4 points) \n",
    "\n",
    "Currently, the work proposed by Taineema et al. performs the extraction of the prefixes\n",
    "of the traces registered in the log to train the classification models. For large logs, this approach\n",
    "leads to an increase in the dimensionality of the models' input (too many features) without\n",
    "necessarily improving its precision, especially in cases in which the event traces are very long.\n",
    "You must modify this technique to extract subsequences of size n (n-grams), where n is a userdefined parameter, instead of encoding entire prefixes. An n-gram is a contiguous sequence of n\n",
    "items from a given trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we define the function that calculates the n-grams. The following function calculates the prefixes using the n-grams for every case separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(data, ngram_size):\n",
    "    result=pd.DataFrame()\n",
    "\n",
    "\n",
    "    for idx in range(0,data.shape[0]- ngram_size +1):\n",
    "\n",
    "        prefix=data.iloc[idx:idx+ngram_size].copy()\n",
    "        prefix=prefix.reset_index()\n",
    "\n",
    "        prefix['Case ID']=prefix['Case ID']+'_'+str(idx)\n",
    "\n",
    "        result=pd.concat([result,prefix])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a helper function, we adapted the following method to the new label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_ratio(data):\n",
    "    class_freqs = data['label'].value_counts()\n",
    "    return class_freqs[1] / class_freqs.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We then follow the same CV method as the practice session 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def get_stratified_split_generator(data, n_splits=5, shuffle=True, random_state=22):\n",
    "    grouped_firsts = data.groupby('Case ID', as_index=False).first()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    for train_index, test_index in skf.split(grouped_firsts, grouped_firsts['label']):\n",
    "        current_train_names = grouped_firsts['Case ID'][train_index]\n",
    "        train_chunk = data[data['Case ID'].isin(current_train_names)].sort_values('time:timestamp', ascending=True, kind='mergesort')\n",
    "        test_chunk = data[~data['Case ID'].isin(current_train_names)].sort_values('time:timestamp', ascending=True, kind='mergesort')\n",
    "        yield (train_chunk, test_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare chunks for CV\n",
    "dt_prefixes = []\n",
    "class_ratios = []\n",
    "min_prefix_length = 1\n",
    "ngram_size=5\n",
    "\n",
    "for train_chunk, test_chunk in get_stratified_split_generator(train, n_splits=n_splits):\n",
    "    class_ratios.append(get_class_ratio(train_chunk))\n",
    "    # generate data where each prefix is a separate instance\n",
    "    dt_prefixes.append(generate_prefix_data(test_chunk, ngram_size))\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "(3 points) \n",
    "\n",
    "Test the results of your modifications with the Turnaround process event log using\n",
    "cluster bucketing, index encoding, and the XGboost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Optimization\n",
    "\n",
    "Taineema et al provide a method for optimizing the model parameters for predictive process monitoring. The file <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/optimize_params.py\">optimize_params.py</a> performs the parameter optimization. We adopted the file by adding the required parameters for the input event log \"turnaround_anon_sla.csv\".\n",
    "\n",
    "We needed also to perform adaptations in the file <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/dataset_confs.py\">dataset_confs.py</a> in order to enable the parameter tuning for the dataset \"turnaround_anon_sla.csv\". \n",
    "\n",
    "We used the following command to execute the optimizer:\n",
    "``` python optimize_params.py turnaround_anon_sla_renamed optimizer_log 10  cluster index xgboost ```\n",
    "\n",
    "The output of the optimizer could be found in the folder \"optimizer_log\". Also, the optimial parameters are in the pickle file <a href=\"https://github.com/Elkoumy/predictive-monitoring-benchmark/blob/master/experiments/dataset_confs.py\">dataset_confs.py</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
